apiVersion: apps/v1
kind: Deployment
metadata:
  name: portfolio-api-quick
  namespace: portfolio
spec:
  replicas: 1
  selector:
    matchLabels:
      app: portfolio-api-quick
  template:
    metadata:
      labels:
        app: portfolio-api-quick
    spec:
      securityContext:
        runAsNonRoot: true
      containers:
      - name: api
        image: python:3.11-slim
        securityContext:
          allowPrivilegeEscalation: false
        ports:
        - containerPort: 8000
        env:
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: portfolio-secrets
              key: OPENAI_API_KEY
        command: ["/bin/sh"]
        args:
        - -c
        - |
          pip install fastapi uvicorn httpx pydantic &&
          cat > /tmp/quick_api.py << 'EOF'
          #!/usr/bin/env python3
          """
          Quick working API for Portfolio - Temporary fix
          Provides basic chat and health endpoints with Sheyla responses
          """
          from fastapi import FastAPI, HTTPException
          from fastapi.middleware.cors import CORSMiddleware
          from pydantic import BaseModel
          import httpx
          import os

          app = FastAPI(title="Portfolio API - Quick Fix")

          # CORS for production
          app.add_middleware(
              CORSMiddleware,
              allow_origins=["https://linksmlm.com", "http://localhost:5173"],
              allow_credentials=True,
              allow_methods=["GET", "POST", "OPTIONS"],
              allow_headers=["*"],
          )

          # Sheyla's responses
          SHEYLA_RESPONSES = {
              "linkops": """LinkOps AI-BOX is Jimmie's flagship project - a conversational AI system specifically designed for property management. It literally plugs into a property manager's computer and immediately understands their data. Within minutes, they can ask 'How many delinquencies do we have this month?' and Jade responds 'We have 5 total. Should I send notices?' When they say 'Yes please,' it automatically generates and sends the notices. It handles work orders, vendor payments, and scheduling the same way.""",
              
              "technical": """Jimmie works across the full stack. For backend: FastAPI, Python, ChromaDB for vector storage. For AI: He uses OpenAI GPT-4o mini for reliable responses. For DevOps: Kubernetes, Docker, Azure cloud services. For frontend: React with TypeScript, TailwindCSS. Everything is designed for production deployment with proper resource limits and monitoring.""",
              
              "devops": """Jimmie has extensive experience with cloud-native deployments. His portfolio platform demonstrates this - it's deployed on Kubernetes with proper health checks, resource limits, and automated testing. He understands security best practices, implements CORS policies, input validation, and secret management.""",
              
              "afterlife": """LinkOps Afterlife is Jimmie's open-source project for digital legacy preservation. Users can upload photos, voice recordings, and personal documents to create an interactive avatar powered by their own knowledge base. It uses D-ID for video synthesis and ElevenLabs for voice, but follows a 'bring-your-own-keys' approach so users maintain complete control over their data.""",
              
              "intro": """Hello! I'm Sheyla, and I'd love to tell you about Jimmie and his innovative AI projects. Jimmie is a technology entrepreneur focused on AI-powered automation solutions. His flagship project is LinkOps AI-BOX with Jade assistant - it's a plug-and-play AI system that connects directly to property managers' computers and helps them manage delinquencies, work orders, and vendor payments through natural conversation. I'm here to answer any questions about his work, experience, or how these solutions can help your business."""
          }

          class ChatRequest(BaseModel):
              message: str
              namespace: str = "portfolio"
              k: int = 3

          class ChatResponse(BaseModel):
              answer: str
              citations: list = []
              model: str = "gpt-4o-mini"
              session_id: str = "demo"
              follow_up_suggestions: list = []
              avatar_info: dict = {}

          class TalkRequest(BaseModel):
              text: str
              avatar_id: str = "default"

          @app.get("/health")
          def health():
              return {
                  "status": "healthy",
                  "service": "portfolio-api-quick",
                  "version": "quick-fix-1.0"
              }

          @app.post("/api/chat", response_model=ChatResponse)
          async def chat(request: ChatRequest):
              message_lower = request.message.lower()
              
              # Determine response based on keywords
              if any(word in message_lower for word in ["linkops", "ai-box", "jade", "property"]):
                  answer = SHEYLA_RESPONSES["linkops"]
                  suggestions = [
                      "How does the AI-BOX work technically?",
                      "What's the business impact and ROI?",
                      "Tell me about other projects"
                  ]
              elif any(word in message_lower for word in ["technical", "technology", "stack", "architecture"]):
                  answer = SHEYLA_RESPONSES["technical"]
                  suggestions = [
                      "What about DevSecOps experience?",
                      "How do you handle scalability?",
                      "Tell me about the deployment strategy"
                  ]
              elif any(word in message_lower for word in ["devops", "kubernetes", "deployment", "infrastructure"]):
                  answer = SHEYLA_RESPONSES["devops"]
                  suggestions = [
                      "What's your CI/CD pipeline?",
                      "How do you handle security?",
                      "Tell me about monitoring and observability"
                  ]
              elif any(word in message_lower for word in ["afterlife", "avatar", "memorial", "legacy"]):
                  answer = SHEYLA_RESPONSES["afterlife"]
                  suggestions = [
                      "How does avatar creation work?",
                      "What about privacy and data control?",
                      "Tell me about the technical implementation"
                  ]
              elif any(word in message_lower for word in ["hello", "hi", "intro", "yourself", "about"]):
                  answer = SHEYLA_RESPONSES["intro"]
                  suggestions = [
                      "Tell me about LinkOps AI-BOX",
                      "What's Jimmie's technical background?",
                      "How can these solutions help my business?"
                  ]
              else:
                  # OpenAI fallback for other questions
                  answer = await get_openai_response(request.message)
                  suggestions = [
                      "Tell me about LinkOps AI-BOX",
                      "What's Jimmie's DevSecOps experience?",
                      "How does the RAG system work?"
                  ]
              
              return ChatResponse(
                  answer=answer,
                  citations=[],
                  model="gpt-4o-mini",
                  session_id="demo-session",
                  follow_up_suggestions=suggestions,
                  avatar_info={
                      "name": "Sheyla",
                      "locale": "en-IN",
                      "description": "Professional Indian lady with warm, simple voice"
                  }
              )

          async def get_openai_response(message: str) -> str:
              """Get response from OpenAI GPT-4o mini"""
              openai_key = os.getenv("OPENAI_API_KEY")
              if not openai_key:
                  return "I'm Sheyla, Jimmie's portfolio assistant. I can tell you about his LinkOps AI-BOX project, his DevSecOps experience, and his AI/ML work. What would you like to know?"
              
              try:
                  async with httpx.AsyncClient() as client:
                      response = await client.post(
                          "https://api.openai.com/v1/chat/completions",
                          headers={
                              "Authorization": f"Bearer {openai_key}",
                              "Content-Type": "application/json"
                          },
                          json={
                              "model": "gpt-4o-mini",
                              "messages": [
                                  {
                                      "role": "system", 
                                      "content": "You are Sheyla, a professional Indian AI assistant representing Jimmie's portfolio. Focus on his LinkOps AI-BOX project for property management and his DevSecOps expertise. Be warm, professional, and technically knowledgeable."
                                  },
                                  {"role": "user", "content": message}
                              ],
                              "max_tokens": 200
                          }
                      )
                      if response.status_code == 200:
                          return response.json()["choices"][0]["message"]["content"]
              except Exception as e:
                  print(f"OpenAI error: {e}")
              
              return "I'm Sheyla, and I can tell you about Jimmie's innovative work with LinkOps AI-BOX and his DevSecOps expertise. What specific area interests you?"

          @app.post("/api/avatar/talk")
          async def avatar_talk(request: TalkRequest):
              return {
                  "url": "",  # No audio generation in quick API
                  "message": request.text,
                  "avatar_id": request.avatar_id,
                  "fallback": "text"
              }

          @app.get("/api/avatar/intro") 
          def avatar_intro():
              return {
                  "message": "Hello! I'm Sheyla, Jimmie's portfolio assistant. I'm excited to tell you about his innovative AI work!",
                  "audio_available": False,
                  "fallback": "text"
              }

          if __name__ == "__main__":
              import uvicorn
              uvicorn.run(app, host="0.0.0.0", port=8000)
          EOF
          python /tmp/quick_api.py

---
apiVersion: v1
kind: Service
metadata:
  name: portfolio-api-quick
  namespace: portfolio
spec:
  selector:
    app: portfolio-api-quick
  ports:
  - port: 80
    targetPort: 8000
  type: ClusterIP