---
# Source: portfolio/templates/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: default
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
    helm.sh/chart: portfolio-0.1.0
    app.kubernetes.io/name: portfolio
    app.kubernetes.io/instance: portfolio
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: portfolio/templates/networkpolicy.yaml
# Default deny-all network policy for namespace
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: portfolio-default-deny
  namespace: default
  labels:
    helm.sh/chart: portfolio-0.1.0
    app.kubernetes.io/name: portfolio
    app.kubernetes.io/instance: portfolio
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress
---
# Source: portfolio/templates/networkpolicy.yaml
# Allow ingress from ingress controller to UI and API
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: portfolio-allow-ingress
  namespace: default
  labels:
    helm.sh/chart: portfolio-0.1.0
    app.kubernetes.io/name: portfolio
    app.kubernetes.io/instance: portfolio
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: portfolio
      app.kubernetes.io/instance: portfolio
  policyTypes:
    - Ingress
  ingress:
    # Allow traffic from ingress controller
    - from:
        - namespaceSelector:
            matchLabels:
              name: ingress-nginx
      ports:
        - protocol: TCP
          port: 8000
        - protocol: TCP
          port: 80
---
# Source: portfolio/templates/networkpolicy.yaml
# Allow API to communicate with ChromaDB
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: portfolio-api-to-chroma
  namespace: default
  labels:
    helm.sh/chart: portfolio-0.1.0
    app.kubernetes.io/name: portfolio
    app.kubernetes.io/instance: portfolio
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: portfolio
      app.kubernetes.io/instance: portfolio
      app.kubernetes.io/component: chroma
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app.kubernetes.io/name: portfolio
              app.kubernetes.io/instance: portfolio
              app.kubernetes.io/component: api
      ports:
        - protocol: TCP
          port: 8000
---
# Source: portfolio/templates/networkpolicy.yaml
# Allow DNS resolution for all pods
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: portfolio-allow-dns
  namespace: default
  labels:
    helm.sh/chart: portfolio-0.1.0
    app.kubernetes.io/name: portfolio
    app.kubernetes.io/instance: portfolio
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: portfolio
      app.kubernetes.io/instance: portfolio
  policyTypes:
    - Egress
  egress:
    # DNS resolution
    - to: []
      ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
---
# Source: portfolio/templates/networkpolicy.yaml
# Allow API to access external services (OpenAI, ElevenLabs, D-ID)
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: portfolio-api-external
  namespace: default
  labels:
    helm.sh/chart: portfolio-0.1.0
    app.kubernetes.io/name: portfolio
    app.kubernetes.io/instance: portfolio
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: portfolio
      app.kubernetes.io/instance: portfolio
      app.kubernetes.io/component: api
  policyTypes:
    - Egress
  egress:
    # HTTPS for external APIs (OpenAI, ElevenLabs, D-ID)
    - to: []
      ports:
        - protocol: TCP
          port: 443
    # HTTP for package downloads during startup (if needed)
    - to: []
      ports:
        - protocol: TCP
          port: 80
    # Allow API to connect to ChromaDB
    - to:
        - podSelector:
            matchLabels:
              app.kubernetes.io/name: portfolio
              app.kubernetes.io/instance: portfolio
              app.kubernetes.io/component: chroma
      ports:
        - protocol: TCP
          port: 8000
---
# Source: portfolio/templates/resourcequota.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: portfolio-quota
  namespace: default
  labels:
    helm.sh/chart: portfolio-0.1.0
    app.kubernetes.io/name: portfolio
    app.kubernetes.io/instance: portfolio
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
spec:
  hard:
    limits.cpu: "4"
    limits.memory: 8Gi
    persistentvolumeclaims: "2"
    requests.cpu: "2"
    requests.memory: 4Gi
    # Additional security-focused limits
    pods: "10"
    secrets: "20"
    configmaps: "20"
    services: "5"
---
# Source: portfolio/templates/resourcequota.yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: portfolio-limits
  namespace: default
  labels:
    helm.sh/chart: portfolio-0.1.0
    app.kubernetes.io/name: portfolio
    app.kubernetes.io/instance: portfolio
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
spec:
  limits:
  # Container limits
  - type: Container
    default:
      cpu: "500m"
      memory: "512Mi"
      ephemeral-storage: "1Gi"
    defaultRequest:
      cpu: "100m"
      memory: "128Mi"
      ephemeral-storage: "100Mi"
    max:
      cpu: "2"
      memory: "2Gi"
      ephemeral-storage: "10Gi"
    min:
      cpu: "10m"
      memory: "64Mi"
      ephemeral-storage: "10Mi"
  
  # Pod limits
  - type: Pod
    max:
      cpu: "4"
      memory: "4Gi"
      ephemeral-storage: "20Gi"
    min:
      cpu: "50m"
      memory: "128Mi"
      ephemeral-storage: "50Mi"
  
  # PVC limits  
  - type: PersistentVolumeClaim
    max:
      storage: "50Gi"
    min:
      storage: "1Gi"
---
# Source: portfolio/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: portfolio
  namespace: default
  labels:
    helm.sh/chart: portfolio-0.1.0
    app.kubernetes.io/name: portfolio
    app.kubernetes.io/instance: portfolio
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: false
---
# Source: portfolio/templates/secret.yaml
# This file will be managed by SOPS in production
# For development, create manually with kubectl or use Helm --set
apiVersion: v1
kind: Secret
metadata:
  name: portfolio-api-secrets
  namespace: default
  labels:
    helm.sh/chart: portfolio-0.1.0
    app.kubernetes.io/name: portfolio
    app.kubernetes.io/instance: portfolio
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  # Base64 encoded secrets - will be managed by external-secrets or SOPS
  OPENAI_API_KEY: ""
  ELEVENLABS_API_KEY: ""
  DID_API_KEY: ""
---
# Source: portfolio/templates/falco-rules.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: portfolio-falco-rules
  namespace: default
  labels:
    helm.sh/chart: portfolio-0.1.0
    app.kubernetes.io/name: portfolio
    app.kubernetes.io/instance: portfolio
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
data:
  portfolio_rules.yaml: |
    # Portfolio-specific Falco security rules

    # Detect privilege escalation attempts
    - rule: Privilege Escalation in Portfolio
      desc: Detect privilege escalation attempts in portfolio containers
      condition: >
        spawned_process and
        container and
        k8s.ns.name = "default" and
        (proc.name in (sudo, su, doas)) and
        not user.name = root
      output: >
        Privilege escalation attempt in portfolio container
        (user=%user.name command=%proc.cmdline container=%container.name
        k8s.pod=%k8s.pod.name)
      priority: WARNING
      tags: [portfolio, privilege_escalation, mitre_privilege_escalation]

    # Detect unexpected file modifications
    - rule: Unauthorized File Modification in Portfolio
      desc: Detect unauthorized file modifications in portfolio containers
      condition: >
        open_write and
        container and
        k8s.ns.name = "default" and
        fd.name startswith /etc and
        not proc.name in (package-manager, apt, yum, apk, npm, pip)
      output: >
        Unauthorized file modification in portfolio container
        (file=%fd.name proc=%proc.name container=%container.name
        k8s.pod=%k8s.pod.name)
      priority: WARNING
      tags: [portfolio, file_modification, persistence]

    # Detect network activity from unexpected processes
    - rule: Unexpected Network Activity in Portfolio
      desc: Detect network connections from unexpected processes
      condition: >
        inbound_outbound and
        container and
        k8s.ns.name = "default" and
        not proc.name in (python, python3, node, nginx, uvicorn, gunicorn)
      output: >
        Unexpected network activity in portfolio container
        (connection=%fd.name proc=%proc.name container=%container.name
        k8s.pod=%k8s.pod.name)
      priority: NOTICE
      tags: [portfolio, network, lateral_movement]

    # Detect shell access in containers (should not happen in production)
    - rule: Shell Access in Portfolio Container
      desc: Detect shell access in portfolio containers
      condition: >
        spawned_process and
        container and
        k8s.ns.name = "default" and
        proc.name in (bash, sh, zsh, fish, csh, tcsh) and
        not proc.pname in (docker, kubectl, containerd, systemd)
      output: >
        Shell access detected in portfolio container
        (shell=%proc.name user=%user.name container=%container.name
        k8s.pod=%k8s.pod.name)
      priority: WARNING
      tags: [portfolio, shell_access, execution]

    # Detect package manager usage (should not happen in production)
    - rule: Package Manager Usage in Portfolio
      desc: Detect package manager usage in portfolio containers
      condition: >
        spawned_process and
        container and
        k8s.ns.name = "default" and
        proc.name in (apt, apt-get, yum, dnf, apk, npm, pip, pip3, pipenv, poetry)
      output: >
        Package manager usage in portfolio container
        (package_manager=%proc.name command=%proc.cmdline container=%container.name
        k8s.pod=%k8s.pod.name)
      priority: WARNING
      tags: [portfolio, package_management, persistence]

    # Detect sensitive file access
    - rule: Sensitive File Access in Portfolio
      desc: Detect access to sensitive files
      condition: >
        open_read and
        container and
        k8s.ns.name = "default" and
        (fd.name startswith /etc/passwd or
         fd.name startswith /etc/shadow or
         fd.name startswith /etc/ssh or
         fd.name contains "id_rsa" or
         fd.name contains "id_dsa" or
         fd.name contains ".pem" or
         fd.name contains "private" or
         fd.name contains "secret" or
         fd.name contains "token")
      output: >
        Sensitive file access in portfolio container
        (file=%fd.name proc=%proc.name container=%container.name
        k8s.pod=%k8s.pod.name)
      priority: ERROR
      tags: [portfolio, sensitive_files, credential_access]

    # Detect crypto mining activity
    - rule: Crypto Mining in Portfolio
      desc: Detect potential crypto mining activity
      condition: >
        spawned_process and
        container and
        k8s.ns.name = "default" and
        (proc.name contains "miner" or
         proc.name contains "xmrig" or
         proc.name contains "cryptonight" or
         proc.cmdline contains "pool" or
         proc.cmdline contains "stratum")
      output: >
        Potential crypto mining activity in portfolio container
        (proc=%proc.name command=%proc.cmdline container=%container.name
        k8s.pod=%k8s.pod.name)
      priority: CRITICAL
      tags: [portfolio, crypto_mining, resource_hijacking]

    # Detect container escape attempts
    - rule: Container Escape Attempt in Portfolio
      desc: Detect potential container escape attempts
      condition: >
        spawned_process and
        container and
        k8s.ns.name = "default" and
        (proc.name in (docker, containerd, runc) or
         proc.cmdline contains "/proc/self/exe" or
         proc.cmdline contains "chroot" or
         proc.cmdline contains "unshare")
      output: >
        Container escape attempt in portfolio container
        (proc=%proc.name command=%proc.cmdline container=%container.name
        k8s.pod=%k8s.pod.name)
      priority: CRITICAL
      tags: [portfolio, container_escape, privilege_escalation]

    # Detect unexpected binary execution
    - rule: Unexpected Binary Execution in Portfolio
      desc: Detect execution of unexpected binaries
      condition: >
        spawned_process and
        container and
        k8s.ns.name = "default" and
        proc.name in (wget, curl, nc, netcat, ncat, socat, nmap, masscan) and
        not proc.pname in (python, python3, node, npm, pip)
      output: >
        Unexpected binary execution in portfolio container
        (binary=%proc.name command=%proc.cmdline container=%container.name
        k8s.pod=%k8s.pod.name)
      priority: WARNING
      tags: [portfolio, unexpected_execution, reconnaissance]
---
# Source: portfolio/templates/securitycontext.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: portfolio-security-config
  namespace: default
  labels:
    helm.sh/chart: portfolio-0.1.0
    app.kubernetes.io/name: portfolio
    app.kubernetes.io/instance: portfolio
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
data:
  security-policy.yaml: |
    apiVersion: kyverno.io/v1
    kind: ClusterPolicy
    metadata:
      name: portfolio-security-policy
    spec:
      validationFailureAction: enforce
      background: true
      rules:
      - name: check-security-context
        match:
          any:
          - resources:
              kinds:
              - Pod
              namespaces:
              - default
        validate:
          message: "Security context must be configured"
          pattern:
            spec:
              securityContext:
                runAsNonRoot: true
                runAsUser: ">0"
              containers:
              - securityContext:
                  allowPrivilegeEscalation: false
                  capabilities:
                    drop:
                    - ALL
                  readOnlyRootFilesystem: "false"  # Allow for our use case
                  runAsNonRoot: true
      
      - name: disallow-privileged
        match:
          any:
          - resources:
              kinds:
              - Pod
              namespaces:
              - default
        validate:
          message: "Privileged containers are not allowed"
          pattern:
            spec:
              containers:
              - securityContext:
                  privileged: "false"
      
      - name: require-resource-limits
        match:
          any:
          - resources:
              kinds:
              - Pod
              namespaces:
              - default
        validate:
          message: "Resource limits are required"
          pattern:
            spec:
              containers:
              - resources:
                  limits:
                    memory: "?*"
                    cpu: "?*"
                  requests:
                    memory: "?*"
                    cpu: "?*"

  falco-rules.yaml: |
    - rule: Detect privileged container
      desc: Detect privileged container in portfolio namespace
      condition: >
        k8s_audit and ka.verb=create and ka.target.pod 
        and ka.target.ns=default
        and container and container.privileged=true
      output: >
        Privileged container created (user=%ka.user.name verb=%ka.verb 
        pod=%ka.target.pod container=%container.name)
      priority: CRITICAL

    - rule: Detect capability additions
      desc: Detect containers with added capabilities
      condition: >
        k8s_audit and ka.verb=create and ka.target.pod
        and ka.target.ns=default
        and container and container.cap_add exists
      output: >
        Container with added capabilities (user=%ka.user.name verb=%ka.verb 
        pod=%ka.target.pod container=%container.name caps=%container.cap_add)
      priority: WARNING

    - rule: Detect write to sensitive directories
      desc: Detect writes to sensitive directories
      condition: >
        open_write and container.ns=default
        and (fd.name startswith /etc or fd.name startswith /root
        or fd.name startswith /var/run or fd.name startswith /usr/bin)
      output: >
        Write to sensitive directory (user=%user.name command=%proc.cmdline 
        file=%fd.name container=%container.name)
      priority: HIGH
---
# Source: portfolio/templates/deployment-chroma.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: portfolio-chroma
  namespace: default
  labels:
    helm.sh/chart: portfolio-0.1.0
    app.kubernetes.io/name: portfolio
    app.kubernetes.io/instance: portfolio
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: chroma
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
---
# Source: portfolio/templates/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: portfolio-data
  namespace: default
  labels:
    helm.sh/chart: portfolio-0.1.0
    app.kubernetes.io/name: portfolio
    app.kubernetes.io/instance: portfolio
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
# Source: portfolio/templates/deployment-chroma.yaml
apiVersion: v1
kind: Service
metadata:
  name: portfolio-chroma
  namespace: default
  labels:
    helm.sh/chart: portfolio-0.1.0
    app.kubernetes.io/name: portfolio
    app.kubernetes.io/instance: portfolio
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: chroma
spec:
  type: ClusterIP
  ports:
    - port: 8000
      targetPort: 8000
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: portfolio
    app.kubernetes.io/instance: portfolio
    app.kubernetes.io/component: chroma
---
# Source: portfolio/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: portfolio-api
  namespace: default
  labels:
    helm.sh/chart: portfolio-0.1.0
    app.kubernetes.io/name: portfolio
    app.kubernetes.io/instance: portfolio
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: api
spec:
  type: ClusterIP
  ports:
    - port: 8000
      targetPort: 8000
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: portfolio
    app.kubernetes.io/instance: portfolio
    app.kubernetes.io/component: api
---
# Source: portfolio/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: portfolio-ui
  namespace: default
  labels:
    helm.sh/chart: portfolio-0.1.0
    app.kubernetes.io/name: portfolio
    app.kubernetes.io/instance: portfolio
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: ui
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: portfolio
    app.kubernetes.io/instance: portfolio
    app.kubernetes.io/component: ui
---
# Source: portfolio/templates/deployment-api.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: portfolio-api
  namespace: default
  labels:
    helm.sh/chart: portfolio-0.1.0
    app.kubernetes.io/name: portfolio
    app.kubernetes.io/instance: portfolio
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: api
spec:
  replicas: 1
  revisionHistoryLimit: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: portfolio
      app.kubernetes.io/instance: portfolio
      app.kubernetes.io/component: api
  template:
    metadata:
      labels:
        app.kubernetes.io/name: portfolio
        app.kubernetes.io/instance: portfolio
        app.kubernetes.io/component: api
    spec:
      serviceAccountName: portfolio
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      initContainers:
        - name: fix-perms
          image: busybox:1.36
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - CHOWN
              - FOWNER
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 0
            runAsNonRoot: false
            runAsUser: 0
            seccompProfile:
              type: RuntimeDefault
          command:
            - sh
            - -c
            - |
              mkdir -p /data/uploads/images /data/uploads/audio /data/chroma /data/cache
              chown -R 10001:10001 /data
              chmod -R 755 /data
          volumeMounts:
            - name: data
              mountPath: /data
      containers:
        - name: api
          image: "ghcr.io/shadow-link-industries/portfolio-api:dev"
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add: []
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 10001
            runAsNonRoot: true
            runAsUser: 10001
            seLinuxOptions:
              type: container_t
            seccompProfile:
              type: RuntimeDefault
            runAsUser: 10001
            runAsGroup: 10001
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /health/live
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /health/ready
              port: http
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 3
          startupProbe:
            httpGet:
              path: /health/ready
              port: http
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 6
          env:
            - name: API_HOST
              value: "0.0.0.0"
            - name: API_PORT
              value: "8000"
            - name: CORS_ALLOW_ORIGINS
              value: "*"
            - name: EMBED_MODEL
              value: "sentence-transformers/all-MiniLM-L6-v2"
            - name: LLM_API_BASE
              value: "https://api.openai.com"
            - name: LLM_MODEL
              value: "gpt-4o-mini"
            - name: LLM_PROVIDER
              value: "openai"
            - name: RAG_NAMESPACE
              value: "portfolio"
            - name: UPLOAD_DIR
              value: "/data/uploads"
            - name: CHROMA_DIR
              value: "/data/chroma"
            - name: SENTENCE_TRANSFORMERS_HOME
              value: "/data/cache"
            - name: TRANSFORMERS_CACHE
              value: "/data/cache"
            - name: DATA_DIR
              value: "/data"
            - name: CHROMA_URL
              value: "http://portfolio-chroma:8000"
            - name: OPENAI_API_KEY
              valueFrom:
                secretKeyRef:
                  name: portfolio-api-secrets
                  key: OPENAI_API_KEY
            - name: ELEVENLABS_API_KEY
              valueFrom:
                secretKeyRef:
                  name: portfolio-api-secrets
                  key: ELEVENLABS_API_KEY
            - name: DID_API_KEY
              valueFrom:
                secretKeyRef:
                  name: portfolio-api-secrets
                  key: DID_API_KEY
          readinessProbe:
            failureThreshold: 12
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 40
            periodSeconds: 5
            timeoutSeconds: 2
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 2
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 200m
              memory: 512Mi
          volumeMounts:
            - name: data
              mountPath: /data
            # Temporary volumes for read-only root filesystem
            - name: tmp
              mountPath: /tmp
            - name: var-tmp
              mountPath: /var/tmp
            - name: var-cache
              mountPath: /var/cache
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: portfolio-data
        # Temporary volumes for read-only root filesystem
        - name: tmp
          emptyDir: {}
        - name: var-tmp
          emptyDir: {}
        - name: var-cache
          emptyDir: {}
---
# Source: portfolio/templates/deployment-chroma.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: portfolio-chroma
  namespace: default
  labels:
    helm.sh/chart: portfolio-0.1.0
    app.kubernetes.io/name: portfolio
    app.kubernetes.io/instance: portfolio
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: chroma
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: portfolio
      app.kubernetes.io/instance: portfolio
      app.kubernetes.io/component: chroma
  template:
    metadata:
      labels:
        app.kubernetes.io/name: portfolio
        app.kubernetes.io/instance: portfolio
        app.kubernetes.io/component: chroma
    spec:
      serviceAccountName: portfolio
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      containers:
        - name: chroma
          image: "chromadb/chroma:0.4.18"
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add: []
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 10001
            runAsNonRoot: true
            runAsUser: 10001
            seLinuxOptions:
              type: container_t
            seccompProfile:
              type: RuntimeDefault
            runAsUser: 10001
            runAsGroup: 10001
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP
          readinessProbe:
            httpGet:
              path: /api/v1/heartbeat
              port: 8000
            initialDelaySeconds: 10
            periodSeconds: 5
          livenessProbe:
            httpGet:
              path: /api/v1/heartbeat
              port: 8000
            initialDelaySeconds: 15
            periodSeconds: 30
          resources:
            requests:
              cpu: "100m"
              memory: "256Mi"
            limits:
              cpu: "500m"
              memory: "1Gi"
          volumeMounts:
            - name: chroma-data
              mountPath: /chroma/chroma
            # Temporary volumes for read-only root filesystem
            - name: tmp
              mountPath: /tmp
            - name: var-tmp
              mountPath: /var/tmp
            - name: var-cache
              mountPath: /var/cache
      volumes:
        - name: chroma-data
          persistentVolumeClaim:
            claimName: portfolio-chroma
        # Temporary volumes for read-only root filesystem
        - name: tmp
          emptyDir: {}
        - name: var-tmp
          emptyDir: {}
        - name: var-cache
          emptyDir: {}
---
# Source: portfolio/templates/deployment-ui.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: portfolio-ui
  namespace: default
  labels:
    helm.sh/chart: portfolio-0.1.0
    app.kubernetes.io/name: portfolio
    app.kubernetes.io/instance: portfolio
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: ui
spec:
  replicas: 1
  revisionHistoryLimit: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: portfolio
      app.kubernetes.io/instance: portfolio
      app.kubernetes.io/component: ui
  template:
    metadata:
      labels:
        app.kubernetes.io/name: portfolio
        app.kubernetes.io/instance: portfolio
        app.kubernetes.io/component: ui
    spec:
      serviceAccountName: portfolio
      securityContext:
        runAsNonRoot: true
        runAsUser: 10001
      containers:
        - name: ui
          image: "ghcr.io/shadow-link-industries/portfolio-ui:dev"
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add: []
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 10001
            runAsNonRoot: true
            runAsUser: 10001
            seLinuxOptions:
              type: container_t
            seccompProfile:
              type: RuntimeDefault
            runAsUser: 10001
            runAsGroup: 10001
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          env:
            - name: VITE_API_BASE_URL
              value: "http://portfolio.localtest.me/api"
          readinessProbe:
            httpGet:
              path: /
              port: 80
            initialDelaySeconds: 10
            periodSeconds: 5
          livenessProbe:
            httpGet:
              path: /
              port: 80
            initialDelaySeconds: 15
            periodSeconds: 30
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          volumeMounts:
            # Temporary volumes for read-only root filesystem
            - name: tmp
              mountPath: /tmp
            - name: var-tmp
              mountPath: /var/tmp
            - name: var-cache
              mountPath: /var/cache
      volumes:
        # Temporary volumes for read-only root filesystem
        - name: tmp
          emptyDir: {}
        - name: var-tmp
          emptyDir: {}
        - name: var-cache
          emptyDir: {}
---
# Source: portfolio/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: portfolio
  namespace: default
  labels:
    helm.sh/chart: portfolio-0.1.0
    app.kubernetes.io/name: portfolio
    app.kubernetes.io/instance: portfolio
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    nginx.ingress.kubernetes.io/cors-allow-headers: Content-Type, Authorization
    nginx.ingress.kubernetes.io/cors-allow-methods: GET, POST, OPTIONS
    nginx.ingress.kubernetes.io/cors-allow-origin: '*'
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
    - host: portfolio.localtest.me
      http:
        paths:
          - path: /api
            pathType: Prefix
            backend:
              service:
                name: portfolio-api
                port:
                  number: 8000
          - path: /
            pathType: Prefix
            backend:
              service:
                name: portfolio-ui
                port:
                  number: 80
---
# Source: portfolio/templates/gatekeeper-constraints.yaml
# Constraint Template for allowed registries
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: k8sallowedregistries
  namespace: default
spec:
  crd:
    spec:
      names:
        kind: K8sAllowedRegistries
      validation:
        openAPIV3Schema:
          type: object
          properties:
            allowedRegistries:
              type: array
              items:
                type: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8sallowedregistries

        violation[{"msg": msg}] {
          input.review.kind.kind == "Pod"
          some i
          image := input.review.object.spec.containers[i].image
          not startswith(image, input.parameters.allowedRegistries[_])
          msg := sprintf("Image '%v' is not from an allowed registry", [image])
        }
---
# Source: portfolio/templates/gatekeeper-constraints.yaml
# Constraint Template for requiring signed images
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: k8srequiresignedimages
  namespace: default
spec:
  crd:
    spec:
      names:
        kind: K8sRequireSignedImages
      validation:
        openAPIV3Schema:
          type: object
          properties:
            exemptNamespaces:
              type: array
              items:
                type: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8srequiresignedimages

        violation[{"msg": msg}] {
          input.review.kind.kind == "Pod"
          not input.review.object.metadata.namespace in input.parameters.exemptNamespaces
          some i
          image := input.review.object.spec.containers[i].image
          startswith(image, "ghcr.io/")
          # This is a simplified check - in production, you'd verify cosign signatures
          not contains(image, "@sha256:")
          msg := sprintf("Image '%v' must be signed and referenced by digest", [image])
        }
---
# Source: portfolio/templates/gatekeeper-constraints.yaml
# Constraint Template for blocking privileged containers
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: k8sblockprivileged
  namespace: default
spec:
  crd:
    spec:
      names:
        kind: K8sBlockPrivileged
      validation:
        openAPIV3Schema:
          type: object
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8sblockprivileged

        violation[{"msg": msg}] {
          input.review.kind.kind == "Pod"
          some i
          container := input.review.object.spec.containers[i]
          container.securityContext.privileged == true
          msg := sprintf("Container '%v' cannot run in privileged mode", [container.name])
        }

        violation[{"msg": msg}] {
          input.review.kind.kind == "Pod"
          some i
          container := input.review.object.spec.containers[i]
          container.securityContext.allowPrivilegeEscalation == true
          msg := sprintf("Container '%v' cannot allow privilege escalation", [container.name])
        }
---
# Source: portfolio/templates/gatekeeper-constraints.yaml
# Constraint Template for requiring resource limits
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: k8srequireresources
  namespace: default
spec:
  crd:
    spec:
      names:
        kind: K8sRequireResources
      validation:
        openAPIV3Schema:
          type: object
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8srequireresources

        violation[{"msg": msg}] {
          input.review.kind.kind == "Pod"
          some i
          container := input.review.object.spec.containers[i]
          not container.resources.limits.memory
          msg := sprintf("Container '%v' must have memory limits", [container.name])
        }

        violation[{"msg": msg}] {
          input.review.kind.kind == "Pod"
          some i
          container := input.review.object.spec.containers[i]
          not container.resources.limits.cpu
          msg := sprintf("Container '%v' must have CPU limits", [container.name])
        }
---
# Source: portfolio/templates/gatekeeper-constraints.yaml
# Constraint for allowed registries
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sAllowedRegistries
metadata:
  name: allowed-registries
  namespace: default
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]
    namespaces: ["default"]
  parameters:
    allowedRegistries:
      - "ghcr.io/"
      - "chromadb/"
      - "busybox:"
---
# Source: portfolio/templates/gatekeeper-constraints.yaml
# Constraint for blocking privileged containers
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sBlockPrivileged
metadata:
  name: block-privileged-containers
  namespace: default
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]
    namespaces: ["default"]
---
# Source: portfolio/templates/gatekeeper-constraints.yaml
# Constraint for requiring resource limits
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequireResources
metadata:
  name: require-resource-limits
  namespace: default
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]
    namespaces: ["default"]
---
# Source: portfolio/templates/gatekeeper-constraints.yaml
# Constraint for requiring signed images
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequireSignedImages
metadata:
  name: require-signed-images
  namespace: default
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]
    namespaces: ["default"]
  parameters:
    exemptNamespaces:
      - "kube-system"
      - "gatekeeper-system"
