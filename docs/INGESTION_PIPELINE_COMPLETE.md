# âœ… Ingestion Pipeline - Complete & Operational

**Date**: November 3, 2025
**Status**: ğŸŸ¢ Production Ready

---

## What We Built

A complete **Ollama-based RAG ingestion pipeline** that:
1. Processes markdown and JSONL files
2. Generates embeddings using Ollama
3. Stores in ChromaDB vector database
4. Enables semantic search for chatbot
5. Auto-moves processed files to archive

---

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DATA INGESTION PIPELINE                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

SOURCE DATA:
/rag-pipeline/new-rag-data/
â”œâ”€â”€ knowledge/*.md
â”œâ”€â”€ presentations/*.md
â””â”€â”€ jimmie/*.md

        â†“ (ingest_to_chroma.py)

PROCESSING STEPS:
1. Read file (markdown/jsonl)
2. Sanitize content (remove HTML, fix encoding)
3. Chunk text (1000 words, 200 overlap)
4. Generate embeddings (Ollama qwen2.5:7b-instruct)
5. Store in ChromaDB with metadata

        â†“

VECTOR DATABASE:
/data/chroma/
â””â”€â”€ portfolio_knowledge collection (39 docs, 3584-dim embeddings)

        â†“

ARCHIVE:
/rag-pipeline/processed-rag-data/
â””â”€â”€ (processed files moved here)

```

---

## Key Components

### 1. Ingestion Script
**Location**: `/rag-pipeline/ingest_to_chroma.py`

**Features**:
- Handles markdown (.md) and JSONL (.jsonl) files
- Ollama embeddings (qwen2.5:7b-instruct, 3584 dimensions)
- Automatic file movement to processed directory
- Maintains directory structure in archive
- Full error handling and logging

**Usage**:
```bash
cd /home/jimmie/linkops-industries/Portfolio/rag-pipeline
python3 ingest_to_chroma.py
```

### 2. RAG Engine (API)
**Location**: `/api/engines/rag_engine.py`

**Updated for Ollama**:
- Uses Ollama `/api/embeddings` endpoint
- Same model as ingestion (qwen2.5:7b-instruct)
- Connects to `portfolio_knowledge` collection
- Semantic search with similarity scoring

**Environment Variables**:
```bash
DATA_DIR=/home/jimmie/linkops-industries/Portfolio/data
OLLAMA_URL=http://localhost:11434
EMBED_MODEL=qwen2.5:7b-instruct
```

### 3. ChromaDB Storage
**Location**: `/data/chroma/`

**Stats**:
- Collection: `portfolio_knowledge`
- Documents: 39 embedded chunks
- Embedding dimension: 3584
- Database size: ~2MB

---

## Ingestion Results

### Files Processed: 28 markdown files

**Categories**:
- **Knowledge**: 21 files (bio, projects, expertise, case studies)
- **Presentations**: 4 files (intro, devops, aiml, afterlife)
- **Miscellaneous**: 3 files (SLA, overview docs)

**Chunks Created**: 39 embedded segments
**Success Rate**: 100% (0 errors)

---

## Data Flow

### Ingestion Time
```
New Data â†’ Sanitize â†’ Chunk â†’ Embed (Ollama) â†’ Store (ChromaDB) â†’ Archive
```

### Query Time
```
User Question â†’ Embed (Ollama) â†’ Search (ChromaDB) â†’ Retrieve Chunks â†’ Claude LLM â†’ Response
```

---

## Testing Results

### âœ… Ingestion Test
```bash
$ python3 ingest_to_chroma.py

âœ… Processed: 28 files
âŒ Errors: 0 files
ğŸ—„ï¸  ChromaDB contains: 39 documents
```

### âœ… RAG Search Test
```python
Query: "What is LinkOps AI-BOX?"

Results:
1. linkops-aibox-technical-deep-dive.md
2. ai-ml-expertise-detailed.md
3. 01-bio.md

âœ… Relevant documents retrieved successfully!
```

---

## File Structure

```
Portfolio/
â”œâ”€â”€ rag-pipeline/
â”‚   â”œâ”€â”€ ingest_to_chroma.py          # âœ¨ NEW ingestion script
â”‚   â”œâ”€â”€ new-rag-data/                # Source files (now empty)
â”‚   â””â”€â”€ processed-rag-data/          # âœ¨ Archived processed files
â”‚       â”œâ”€â”€ knowledge/               # 21 files
â”‚       â”œâ”€â”€ presentations/           # 4 files
â”‚       â””â”€â”€ jimmie/                  # 1 file
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ chroma/                      # âœ¨ Vector database
â”‚   â”‚   â”œâ”€â”€ chroma.sqlite3           # 2MB database
â”‚   â”‚   â””â”€â”€ portfolio_knowledge/    # Collection data
â”‚   â””â”€â”€ knowledge/                   # âš ï¸ Can be deleted (legacy)
â”‚
â””â”€â”€ api/
    â””â”€â”€ engines/
        â””â”€â”€ rag_engine.py            # âœ¨ UPDATED for Ollama
```

---

## How to Use

### Adding New Data

1. **Place files** in `/rag-pipeline/new-rag-data/`
   ```bash
   cp new_document.md rag-pipeline/new-rag-data/knowledge/
   ```

2. **Run ingestion**
   ```bash
   cd rag-pipeline
   python3 ingest_to_chroma.py
   ```

3. **Files automatically moved** to `processed-rag-data/` after ingestion

### Querying via API

```python
from api.engines.rag_engine import RAGEngine

# Initialize
rag = RAGEngine()

# Search
results = rag.search("What projects has Jimmie worked on?", n_results=5)

# Results include text + metadata
for result in results:
    print(result['text'])
    print(result['metadata'])
```

### Testing Search

```bash
DATA_DIR=/home/jimmie/linkops-industries/Portfolio/data python3 -c "
from api.engines.rag_engine import RAGEngine
rag = RAGEngine()
results = rag.search('Your question here')
print(f'Found {len(results)} results')
"
```

---

## Technical Details

### Ollama Embeddings

**Model**: qwen2.5:7b-instruct
**API Endpoint**: `http://localhost:11434/api/embeddings`
**Embedding Dimension**: 3584
**Response Time**: ~100-200ms per embedding

**API Call**:
```python
requests.post(
    "http://localhost:11434/api/embeddings",
    json={"model": "qwen2.5:7b-instruct", "prompt": text}
)
```

### ChromaDB Configuration

**Type**: Persistent (SQLite)
**Path**: `/data/chroma/`
**Collection**: `portfolio_knowledge`
**Metadata Fields**: source, chunk_index, file_type, ingestion_date

---

## Supported File Formats

### Markdown (.md)
- Standard markdown syntax
- Headers, lists, code blocks
- Automatically chunked by word count

### JSONL (.jsonl)
- One JSON object per line
- Flexible field names: `text`, `content`, `document`
- Additional fields stored as metadata

**Example JSONL**:
```jsonl
{"text": "Document content here", "author": "Jimmie", "date": "2025-11-03"}
{"content": "Another document", "tags": ["ai", "ml"]}
```

---

## Environment Configuration

### Required Environment Variables

```bash
# Data directory (where chroma/ lives)
export DATA_DIR=/home/jimmie/linkops-industries/Portfolio/data

# Ollama API endpoint
export OLLAMA_URL=http://localhost:11434

# Embedding model (must match ingestion model)
export EMBED_MODEL=qwen2.5:7b-instruct
```

### Optional Variables

```bash
# RAG namespace for collection naming
export RAG_NAMESPACE=portfolio

# Chunking configuration
export CHUNK_SIZE=1000
export CHUNK_OVERLAP=200
```

---

## Maintenance

### Re-ingesting Data

If you need to rebuild the entire database:

```bash
# 1. Backup current database
cp -r data/chroma data/chroma.backup

# 2. Clear ChromaDB
rm -rf data/chroma/*

# 3. Restore files from processed archive
cp -r rag-pipeline/processed-rag-data/* rag-pipeline/new-rag-data/

# 4. Re-run ingestion
cd rag-pipeline && python3 ingest_to_chroma.py
```

### Viewing ChromaDB Contents

```python
import chromadb

client = chromadb.PersistentClient(path="./data/chroma")
collection = client.get_collection("portfolio_knowledge")

# Get count
print(f"Documents: {collection.count()}")

# Get all documents
all_docs = collection.get()
for doc_id, text, metadata in zip(all_docs['ids'], all_docs['documents'], all_docs['metadatas']):
    print(f"{metadata['source']}: {text[:100]}...")
```

---

## Next Steps

### Integration with Chatbot

The RAG engine is now ready for use in your chatbot:

1. **Start API server**
   ```bash
   cd api
   DATA_DIR=/home/jimmie/linkops-industries/Portfolio/data uvicorn main:app --reload
   ```

2. **Chat endpoint** at `/api/chat` will:
   - Receive user question
   - Use RAG engine to search ChromaDB
   - Send context + question to Claude
   - Return grounded response with citations

### AWS Landing Zone (Next Phase)

Now that local ingestion works, you can build the AWS LocalStack version:

- S3 buckets for raw/processed/failed data
- Lambda functions for processing
- DynamoDB for metadata tracking
- SQS for job queuing
- EventBridge for automation

---

## Troubleshooting

### "Collection expecting embedding with dimension X"

**Problem**: Dimension mismatch between ingestion and query embeddings

**Solution**: Ensure same model used for both
```bash
# Check ingestion model
grep "embedding_model" rag-pipeline/ingest_to_chroma.py

# Check RAG engine model
grep "embed_model" api/engines/rag_engine.py
```

### "Permission denied" errors

**Problem**: DATA_DIR points to root /data/ instead of project directory

**Solution**: Set environment variable
```bash
export DATA_DIR=/home/jimmie/linkops-industries/Portfolio/data
```

### Ollama 404 errors

**Problem**: Wrong API endpoint or model not available

**Solution**:
```bash
# Check Ollama is running
ollama list

# Test embedding endpoint
curl -X POST http://localhost:11434/api/embeddings \
  -d '{"model":"qwen2.5:7b-instruct","prompt":"test"}'
```

---

## Performance

### Ingestion Speed
- **28 files**: ~3-5 minutes
- **Per chunk**: ~0.2-0.5 seconds (Ollama embedding)
- **Bottleneck**: Embedding generation (CPU-bound)

### Query Speed
- **Search time**: <200ms
- **Embedding generation**: ~100-200ms
- **ChromaDB query**: <50ms
- **Total**: <300ms for semantic search

---

## Summary

âœ… **Ingestion pipeline**: Built from scratch, Ollama-based
âœ… **File processing**: Markdown + JSONL support
âœ… **Vector database**: 39 documents in ChromaDB
âœ… **RAG engine**: Updated for Ollama embeddings
âœ… **File archival**: Auto-move to processed directory
âœ… **Testing**: Semantic search working correctly

**Ready for production chatbot integration!** ğŸš€

---

## Commands Quick Reference

```bash
# Run ingestion
cd rag-pipeline && python3 ingest_to_chroma.py

# Test RAG search
DATA_DIR=$PWD/data python3 -c "
from api.engines.rag_engine import RAGEngine
rag = RAGEngine()
print(rag.search('test query'))
"

# Check ChromaDB
python3 -c "
import chromadb
client = chromadb.PersistentClient(path='./data/chroma')
col = client.get_collection('portfolio_knowledge')
print(f'Documents: {col.count()}')
"

# Start API server
cd api && DATA_DIR=$PWD/../data uvicorn main:app --reload --port 8000
```

---

**Pipeline Status**: âœ… Complete and operational
**Next Phase**: AWS LocalStack integration for certification demo
