# API Engines Analysis - What's Used vs What's Dead Code

**Date**: November 3, 2025
**Status**: Analysis Complete

---

## Summary Table

| Engine File | Status | Used By | Purpose |
|------------|--------|---------|---------|
| **llm_interface.py** | âœ… **ACTIVE** | routes/chat.py | **Claude API integration** - Main LLM engine |
| **rag_engine.py** | âœ… **ACTIVE** | routes/chat.py, routes/rag.py, routes/actions.py | **ChromaDB queries** - Semantic search |
| **llm_engine.py** | âš ï¸ DUPLICATE | routes/chat.py | Old local LLM engine (Qwen/GPT fallback) |
| **jade_engine.py** | âŒ UNUSED | None | Old conversation engine (not called) |
| **rag_interface.py** | âŒ UNUSED | None | Old RAG interface (duplicate functionality) |
| **response_generator.py** | âŒ UNUSED | None | Old response generator (not called) |
| **avatar_engine.py** | âŒ STUB | None | Avatar video generation (stub only) |
| **speech_engine.py** | âŒ STUB | None | Text-to-speech (stub only) |

---

## Detailed Analysis

### âœ… ACTIVE ENGINES (Keep These)

#### 1. **llm_interface.py** - PRIMARY LLM ENGINE

**Purpose**: Claude API integration with multi-provider support

**What it does**:
- Manages Claude 3.5 Sonnet API calls
- Streams responses from Claude
- Falls back to OpenAI GPT-4o-mini
- Can use local Qwen model if needed

**Code snippet**:
```python
class LLMEngine:
    def __init__(self):
        self.provider = os.getenv("LLM_PROVIDER", "claude")

        if self.provider == "claude":
            self.claude_api_key = os.getenv("CLAUDE_API_KEY")
            self.claude_model = "claude-3-5-sonnet-20241022"

    async def _generate_claude(self, prompt, max_tokens):
        """Generate using Claude API with streaming"""
        from anthropic import AsyncAnthropic
        client = AsyncAnthropic(api_key=self.claude_api_key)

        async with client.messages.stream(...) as stream:
            async for text in stream.text_stream:
                yield text
```

**Used by**:
- `routes/chat.py` â†’ `get_llm_engine()` â†’ Generates chat responses

**Status**: âœ… **ACTIVELY USED - KEEP THIS**

---

#### 2. **rag_engine.py** - SEMANTIC SEARCH ENGINE

**Purpose**: ChromaDB vector search with Ollama embeddings

**What it does**:
- Connects to ChromaDB (`/data/chroma/`)
- Generates query embeddings using Ollama (`nomic-embed-text`)
- Searches for semantically similar documents
- Returns top N relevant chunks with metadata

**Code snippet**:
```python
class RAGEngine:
    def __init__(self):
        self.client = chromadb.PersistentClient(path=chroma_dir)
        self.collection = self._get_active_collection()
        self.ollama_url = "http://localhost:11434"
        self.embed_model = "nomic-embed-text"

    def search(self, query: str, n_results: int = 5):
        """Search for relevant documents"""
        # 1. Get query embedding from Ollama
        query_embedding = self._get_embedding(query)

        # 2. Search ChromaDB
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results
        )

        # 3. Return formatted results
        return contexts
```

**Used by**:
- `routes/chat.py` â†’ `get_rag_engine()` â†’ Retrieves context for questions
- `routes/rag.py` â†’ RAG ingestion and search endpoints
- `routes/actions.py` â†’ RAG operations

**Status**: âœ… **ACTIVELY USED - KEEP THIS**

---

### âš ï¸ DUPLICATE/OLD ENGINES (Can Delete)

#### 3. **llm_engine.py** - OLD LLM ENGINE

**Purpose**: Legacy local LLM engine (before Claude integration)

**What it does**:
- Loads HuggingFace Qwen model locally
- OpenAI GPT-4o-mini fallback
- Streaming generation

**Why it's duplicate**:
- `llm_interface.py` does everything this does, plus Claude
- Same function signatures
- Both have `generate()` method
- Both support OpenAI and local models

**Code comparison**:
```python
# llm_engine.py (OLD)
class LLMEngine:
    def __init__(self):
        self.engine = os.getenv("LLM_ENGINE", "local")  # Old variable
        # Only supports local or openai

# llm_interface.py (NEW) âœ…
class LLMEngine:
    def __init__(self):
        self.provider = os.getenv("LLM_PROVIDER", "claude")  # New variable
        # Supports claude, openai, AND local
```

**Used by**:
- `routes/chat.py` imports it, but `llm_interface.py` is what actually gets used

**Status**: âš ï¸ **DUPLICATE - CAN DELETE** (llm_interface.py is better)

---

#### 4. **jade_engine.py** - OLD CONVERSATION ENGINE

**Purpose**: Avatar personality and conversation management

**What it does**:
- Loads Sheyla/Jade personality config
- Manages conversation context
- Tracks mentioned projects
- Pre-prepared Q&A responses

**Why it's unused**:
- Code references "Sheyla" avatar (removed)
- `routes/chat.py` has stub placeholder for it
- Never actually instantiated or called
- Functionality now in `llm_interface.py` + prompts

**Code snippet**:
```python
class ConversationEngine:
    def __init__(self, chat_data_dir="chat/data"):
        self.personality = self._load_personality()  # Loads Sheyla config
        self.qa_database = self._load_qa_database()

    def _load_personality(self):
        return {
            "name": "Sheyla",
            "heritage": "Indian",
            "role": "Portfolio representative"
        }
```

**Used by**: NONE (stub only)

**Status**: âŒ **UNUSED - DELETE THIS**

---

#### 5. **rag_interface.py** - OLD RAG INTERFACE

**Purpose**: Legacy RAG interface using sentence-transformers

**What it does**:
- ChromaDB queries with sentence-transformers embeddings
- Uses `sentence-transformers/all-MiniLM-L6-v2` (384D)
- Config file based setup

**Why it's duplicate**:
- `rag_engine.py` does the same thing but better
- Uses old embedding model (384D vs 768D)
- Depends on external config files
- No Ollama support

**Code comparison**:
```python
# rag_interface.py (OLD) - 384D embeddings
class RAGInterface:
    def __init__(self):
        self.model = SentenceTransformer("all-MiniLM-L6-v2")  # 384D

# rag_engine.py (NEW) âœ… - 768D embeddings
class RAGEngine:
    def __init__(self):
        self.embed_model = "nomic-embed-text"  # 768D via Ollama
```

**Used by**: NONE

**Status**: âŒ **UNUSED - DELETE THIS**

---

#### 6. **response_generator.py** - OLD RESPONSE GENERATOR

**Purpose**: Legacy response generation with Jade personality

**What it does**:
- Combines RAG context + LLM
- Applies Jade personality
- Creates system prompts

**Why it's unused**:
- This logic now in `routes/chat.py` directly
- References old `rag_interface.py`
- Uses old config system
- Personality now in system prompts

**Code snippet**:
```python
class ResponseGenerator:
    def generate_response(self, user_question):
        # 1. Get RAG context
        context = self.rag.get_context(user_question)

        # 2. Create system prompt (Jade personality)
        system_prompt = self._create_system_prompt()

        # 3. Call LLM
        response = self._call_llm(system_prompt, user_prompt)
```

**Used by**: NONE

**Status**: âŒ **UNUSED - DELETE THIS**

---

### âŒ STUB ENGINES (Keep or Delete)

#### 7. **avatar_engine.py** - AVATAR VIDEO STUB

**Purpose**: D-ID avatar video generation

**What it does**:
- Stub implementation (returns placeholder)
- Would generate talking avatar videos
- Uses D-ID API (you have key)

**Code**:
```python
class AvatarEngine:
    def generate_video(self, script, image_path):
        """Generate avatar video (stub implementation)"""
        # For now, return a placeholder response
        return "data:text/plain;base64,VGhpcyBpcyBhIHBsYWNlaG9sZGVy..."
```

**Used by**: NONE (no 3D avatars anymore)

**Status**: âŒ **STUB ONLY**
- **Keep if**: You want avatar videos in the future
- **Delete if**: You're staying with simple chatbox

---

#### 8. **speech_engine.py** - TEXT-TO-SPEECH STUB

**Purpose**: ElevenLabs text-to-speech

**What it does**:
- Stub implementation (returns placeholder)
- Would generate voice audio
- Uses ElevenLabs API (you have key)

**Code**:
```python
class SpeechEngine:
    def text_to_speech(self, text):
        """Convert text to speech (stub implementation)"""
        # For now, return a placeholder response
        return "data:audio/wav;base64,UklGRkQDAABXQVZF..."
```

**Used by**: NONE

**Status**: âŒ **STUB ONLY**
- **Keep if**: You want voice responses in the future
- **Delete if**: Text-only chatbox is sufficient

---

## Current Data Flow (What Actually Runs)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ USER: "What is LinkOps AI-BOX?"         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ routes/chat.py                           â”‚
â”‚ POST /chat endpoint                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ rag_engine.py (ACTIVE)                   â”‚
â”‚ 1. Embed query with Ollama               â”‚
â”‚    (nomic-embed-text, 768D)              â”‚
â”‚ 2. Search ChromaDB                       â”‚
â”‚ 3. Return top 5 chunks                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ llm_interface.py (ACTIVE)                â”‚
â”‚ 1. Build prompt with RAG context         â”‚
â”‚ 2. Call Claude API                       â”‚
â”‚    (claude-3-5-sonnet-20241022)          â”‚
â”‚ 3. Stream response back                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RESPONSE (with citations)                â”‚
â”‚ Returns to user via UI                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ENGINES NOT USED:
âŒ jade_engine.py
âŒ rag_interface.py
âŒ response_generator.py
âŒ avatar_engine.py (stub)
âŒ speech_engine.py (stub)
âš ï¸  llm_engine.py (duplicate)
```

---

## Recommendations

### Option A: Aggressive Cleanup (Recommended)

**Delete these files:**
```bash
rm api/engines/jade_engine.py           # Old conversation engine
rm api/engines/rag_interface.py         # Old RAG (duplicate)
rm api/engines/response_generator.py    # Old response gen
rm api/engines/llm_engine.py            # Duplicate of llm_interface.py
```

**Keep these files:**
```bash
# Active engines
api/engines/llm_interface.py  âœ… Claude + multi-provider
api/engines/rag_engine.py     âœ… ChromaDB + Ollama embeddings

# Stubs (if you might use later)
api/engines/avatar_engine.py  âš ï¸ (stub)
api/engines/speech_engine.py  âš ï¸ (stub)
```

**Result**: 4 files deleted, 2 active + 2 stubs remaining

---

### Option B: Conservative Cleanup

**Delete obviously unused:**
```bash
rm api/engines/jade_engine.py           # Sheyla avatar (removed)
rm api/engines/rag_interface.py         # Duplicate RAG
rm api/engines/response_generator.py    # Old response gen
```

**Keep everything else** (including llm_engine.py as backup)

**Result**: 3 files deleted, 5 remaining

---

### Option C: Delete Stubs Too (Maximum Cleanup)

**Delete everything unused:**
```bash
rm api/engines/jade_engine.py
rm api/engines/rag_interface.py
rm api/engines/response_generator.py
rm api/engines/llm_engine.py
rm api/engines/avatar_engine.py         # Stub only
rm api/engines/speech_engine.py         # Stub only
```

**Keep only active:**
```bash
api/engines/llm_interface.py  âœ…
api/engines/rag_engine.py     âœ…
```

**Result**: 6 files deleted, 2 remaining (cleanest!)

---

## Verification

To verify which engines are actually imported/used:

```bash
# Check what's imported in routes
grep -r "from engines" api/routes/

# Output shows:
# routes/chat.py:    from engines.rag_engine import RAGEngine
# routes/chat.py:    from engines.llm_engine import LLMEngine
# routes/rag.py:     from engines.rag_engine import get_rag_engine
```

**Actual usage**:
- `rag_engine.py` â†’ Used by chat, rag, and actions routes âœ…
- `llm_engine.py` â†’ Imported but llm_interface.py is what runs âš ï¸
- Everything else â†’ Not imported anywhere âŒ

---

## My Recommendation

**Go with Option A: Aggressive Cleanup**

Delete these 4 files:
1. `jade_engine.py` - Old Sheyla avatar engine
2. `rag_interface.py` - Duplicate of rag_engine.py
3. `response_generator.py` - Logic now in routes/chat.py
4. `llm_engine.py` - Duplicate of llm_interface.py

Keep these 4 files:
1. `llm_interface.py` - âœ… Active (Claude API)
2. `rag_engine.py` - âœ… Active (ChromaDB)
3. `avatar_engine.py` - âš ï¸ Stub (future use)
4. `speech_engine.py` - âš ï¸ Stub (future use)

**Why**:
- Removes all dead code
- Keeps active engines
- Keeps stubs in case you want avatar/voice later
- Clean and minimal

**Command to execute**:
```bash
cd /home/jimmie/linkops-industries/Portfolio/api/engines
rm jade_engine.py rag_interface.py response_generator.py llm_engine.py
```

---

## After Cleanup

Your `api/engines/` will contain:

```
api/engines/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ llm_interface.py       âœ… Claude integration
â”œâ”€â”€ rag_engine.py          âœ… ChromaDB queries
â”œâ”€â”€ avatar_engine.py       âš ï¸ Stub (optional)
â””â”€â”€ speech_engine.py       âš ï¸ Stub (optional)
```

**Clean, focused, production-ready!** ğŸš€
